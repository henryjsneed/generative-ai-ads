{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "sys.path.append(\"/home/glaurung/ai-ads\")\n",
    "sys.path.append(\"/home/glaurung/ai-ads/dlrm\")\n",
    "from dlrm import data_utils\n",
    "import dlrm\n",
    "import pickle\n",
    "from dlrm_s_pytorch import DLRM_Net\n",
    "import numpy as np\n",
    "import ad_copy_util\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### this section is for POC model deployment. It's not useful for testing because you still need ctr for the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Create a list of 10 catchy phrases that could be used in an advertisement for a new sports drink flavor called Jungle Torrent targeting 20 year old athletes.\"\n",
    "# ad_copy_options = ['1. \"Thirst to Win with Jungle Torrent!\"', '2. \"Start your Winning Streak with Jungle Torrent!\" ', '3. \"Outperform with Jungle Torrent!\"', '4. \"Hydrate to Dominate with Jungle Torrent!\"', '5. \"Stay Energized and Go the Distance with Jungle Torrent!\"', '6. \"Recharge with Jungle Torrent!\"', '7. \"Beat Your Best with Jungle Torrent!\"', '8. \"Go Wild with Jungle Torrent!\"', '9. \"Outpace the Competition with Jungle Torrent!\"', '10. \"Unlock Your Potential with Jungle Torrent!\"']\n",
    "# #ad_copy_util.generate_ad_copy_options(prompt, max_items=10, max_tokens=300, temperature=1)\n",
    "# print(ad_copy_options)\n",
    "\n",
    "# # Use BertModel to generate embeddings instead of OpenAI API to save time and credits.\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"using \", device)\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "# ad_copy_embeddings = ad_copy_util.generate_text_embeddings(ad_copy_options, model, tokenizer, device)\n",
    "# for i, embedding in enumerate(ad_copy_embeddings):\n",
    "#     print(f\"Embedding {i+1} shape: {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare categorical test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {\n",
    "    'ad_id': LabelEncoder(),\n",
    "    'device_type': LabelEncoder(),\n",
    "    'location': LabelEncoder(),\n",
    "    'browser': LabelEncoder(),  \n",
    "    'content_category': LabelEncoder(),\n",
    "    'ad_copy': LabelEncoder(),\n",
    "    'product_type': LabelEncoder(),\n",
    "    'ad_type': LabelEncoder(),\n",
    "    'time_of_day': LabelEncoder(),\n",
    "    'day_of_week': LabelEncoder(),\n",
    "    'interaction_type': LabelEncoder(),\n",
    "    'historical_ad_category': LabelEncoder()\n",
    "}\n",
    "\n",
    "df_categorical_test = common.transform_with_label_encoders(label_encoders, df_test)\n",
    "\n",
    "categorical_features = [tuple(values) for values in df_categorical_test.to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare continuous test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "continuous_fields = ['age', 'site_visit_duration', 'time_spent_on_ad', 'pages_visited_this_session','ads_viewed_last_month', 'avg_time_spent_on_clicked_ads', 'site_visit_frequency']\n",
    "\n",
    "df_continuous = common.load_and_transform_scaler(continuous_fields, df_test)\n",
    "\n",
    "continuous_features = [tuple(values) for values in df_continuous.to_numpy()]\n",
    "\n",
    "target_feature = df_test['ad_clicked'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve ad copy embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = 'ad_copy_embeddings.pkl'\n",
    "with open(embeddings_file, 'rb') as file:\n",
    "    ad_copy_embeddings_dict = pickle.load(file)\n",
    "continuous_features_flat = common.prepare_continuous_features_with_embeddings(df_test, df_continuous, ad_copy_embeddings_dict,'ad_copy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the DLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DLRM_Net(\n",
       "  (emb_l): ModuleList(\n",
       "    (0): EmbeddingBag(84, 775, mode='sum')\n",
       "    (1): EmbeddingBag(3, 775, mode='sum')\n",
       "    (2): EmbeddingBag(5, 775, mode='sum')\n",
       "    (3): EmbeddingBag(4, 775, mode='sum')\n",
       "    (4): EmbeddingBag(6, 775, mode='sum')\n",
       "    (5): EmbeddingBag(84, 775, mode='sum')\n",
       "    (6): EmbeddingBag(6, 775, mode='sum')\n",
       "    (7-8): 2 x EmbeddingBag(4, 775, mode='sum')\n",
       "    (9): EmbeddingBag(7, 775, mode='sum')\n",
       "    (10): EmbeddingBag(3, 775, mode='sum')\n",
       "    (11): EmbeddingBag(6, 775, mode='sum')\n",
       "  )\n",
       "  (bot_l): Sequential()\n",
       "  (top_l): Sequential(\n",
       "    (0): Linear(in_features=775, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (loss_fn): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_cardinalities = []\n",
    "\n",
    "# Loop through each category feature and calculate the cardinality\n",
    "for column in label_encoders.keys():\n",
    "    cardinality = len(df_test[column].unique())\n",
    "    category_cardinalities.append(cardinality)\n",
    "\n",
    "category_cardinalities_array = np.array(category_cardinalities)\n",
    "\n",
    "# embedding_sizes: the sizes of the embedding tables based on the cardinalities of the categorical features\n",
    "ln_emb = category_cardinalities_array\n",
    "\n",
    "# original number of continuous features\n",
    "original_m_spa = np.array(continuous_features[0]).shape[0]\n",
    "\n",
    "# size of each ad copy embedding\n",
    "ad_copy_embedding_size = 768  \n",
    "\n",
    "# m_spa is the size of each embedding\n",
    "m_spa = original_m_spa + ad_copy_embedding_size\n",
    "\n",
    "ln_bot = np.array([m_spa])\n",
    "\n",
    "# ln_top = np.array([m_spa + embedding_size * len(categorical_features[0]), 16, 1])\n",
    "ln_top = np.array([775, 16, 1])\n",
    "\n",
    "device = \"cpu\"\n",
    "model = DLRM_Net(\n",
    "    m_spa,\n",
    "    ln_emb,\n",
    "    ln_bot,\n",
    "    ln_top,\n",
    "    arch_interaction_op=\"dot\",\n",
    "    sigmoid_bot=-1,\n",
    "    sigmoid_top=len(ln_top) - 2,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"/home/glaurung/ai-ads/trained_model.pt\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = torch.tensor(categorical_features, dtype=torch.long)\n",
    "X_cont = torch.tensor(continuous_features_flat, dtype=torch.float32)\n",
    "Y = torch.tensor(target_feature, dtype=torch.float32).view(-1, 1)\n",
    "dataset = TensorDataset(X_cont, X_cat, Y)\n",
    "\n",
    "# Create dataset and data loader\n",
    "test_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.6951\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "test_loss = 0.0\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "with torch.no_grad():\n",
    "    for x_cont, x_cat, y in test_loader:\n",
    "        lS_o, lS_i = common.generate_offsets_and_indices_per_feature(x_cat)\n",
    "        y_pred = model(x_cont, lS_o, lS_i)\n",
    "        loss = criterion(y_pred, y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Average Test Loss: {avg_test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
