{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import joblib\n",
    "from model.DLRM_Net import DLRM_Net\n",
    "from model.DLRM_Dataset import DLRM_Dataset\n",
    "import torch.nn as nn\n",
    "import common\n",
    "import ad_copy_util\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_artifacts/label_encoders.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train/train_data.csv')\n",
    "\n",
    "categorical_cols = ['location', 'product_type', 'ad_type']\n",
    "label_encoders = {}\n",
    "encoded_categorical_data = np.empty((df.shape[0], len(categorical_cols)))\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    le = LabelEncoder()\n",
    "    encoded_categorical_data[:, i] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "joblib.dump(label_encoders, 'model_artifacts/label_encoders.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Generate Ad Copy Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_copy_file = '../preprocessing/data/ad_copy.json'\n",
    "embeddings_file = 'model_artifacts/ad_copy_embeddings.pkl'\n",
    "if os.path.exists(embeddings_file):\n",
    "    with open(embeddings_file, 'rb') as file:\n",
    "        ad_copy_embeddings_dict = pickle.load(file)\n",
    "else:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    ad_copy_embeddings_dict = common.generate_all_embeddings(ad_copy_file, model, tokenizer, device, 1024)\n",
    "    \n",
    "# this maps embeddings to ad_copy\n",
    "embeddings_list = df['ad_copy'].map(ad_copy_embeddings_dict).tolist()\n",
    "ad_copy_embeddings = np.vstack(embeddings_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11563431 -0.08019764 -0.14963417 ... -0.510401    0.13010667\n",
      "   0.20229894]\n",
      " [-0.05821146  0.27160457  0.01331483 ... -0.5414637   0.13168468\n",
      "   0.06821588]\n",
      " [ 0.07516024 -0.03162972  0.08582542 ... -0.4126556   0.19505903\n",
      "   0.04414247]\n",
      " ...\n",
      " [-0.18419725 -0.173914   -0.14313711 ... -0.38819295 -0.29105833\n",
      "   0.51544636]\n",
      " [-0.13988926 -0.13345717 -0.24335426 ... -0.48182994 -0.02182761\n",
      "   0.3863272 ]\n",
      " [ 0.11424109 -0.03569167 -0.02830822 ... -0.33884987  0.06618364\n",
      "   0.25962827]]\n"
     ]
    }
   ],
   "source": [
    "print(ad_copy_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA to Ad Copy Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "reduced_embeddings = pca.fit_transform(ad_copy_embeddings)\n",
    "\n",
    "pca_model_path = 'model_artifacts/pca_model.pkl'\n",
    "with open(pca_model_path, 'wb') as file:\n",
    "    pickle.dump(pca, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.49451908 -1.2166103  -2.394447   ...  0.2286228   0.34895322\n",
      "  -0.32854813]\n",
      " [ 3.160706   -2.648871   -1.7516261  ... -0.09235296  0.0289941\n",
      "  -0.74444884]\n",
      " [ 2.2588067   2.4183207  -1.3575181  ... -0.50542367 -0.5700436\n",
      "  -0.06037248]\n",
      " ...\n",
      " [ 0.37339655  3.7028925   1.5538087  ...  0.65753716  0.11671749\n",
      "   0.44288847]\n",
      " [-2.6146228  -2.4884183   1.1762469  ...  0.41475865 -0.26969522\n",
      "   0.49320617]\n",
      " [-0.9713625   2.0740376   1.8903064  ...  0.08653836 -0.4026144\n",
      "  -0.32249522]]\n"
     ]
    }
   ],
   "source": [
    "print(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Ad Copy Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_scaler = common.fit_and_save_scaler(reduced_embeddings, 'model_artifacts/embeddings_scaler.pkl')\n",
    "scaled_ad_copy_embeddings = common.load_and_transform_scaler(reduced_embeddings, 'model_artifacts/embeddings_scaler.pkl')\n",
    "\n",
    "scaled_embeddings_path = 'model_artifacts/scaled_ad_copy_embeddings.pkl'\n",
    "with open(scaled_embeddings_path, 'wb') as file:\n",
    "    pickle.dump(scaled_ad_copy_embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3006587  0.29868007 0.02697474 ... 0.5685825  0.64071965 0.32893556]\n",
      " [0.6406016  0.09449828 0.11362825 ... 0.4638026  0.49596602 0.13467014]\n",
      " [0.55672324 0.8168726  0.16675478 ... 0.3289591  0.22495353 0.45419925]\n",
      " ...\n",
      " [0.3813765  1.         0.5592073  ... 0.70859796 0.53565323 0.68927026]\n",
      " [0.10348502 0.11737227 0.5083113  ... 0.62934494 0.36083508 0.71277344]\n",
      " [0.25631136 0.7677919  0.6045679  ... 0.52220017 0.3007007  0.33176285]]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_ad_copy_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Continuous Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glaurung/.local/lib/python3.10/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "continuous_fields = ['age', 'site_visit_frequency']\n",
    "df_continuous = df[continuous_fields]\n",
    "continuous_scaler = common.fit_and_save_scaler(df_continuous, 'model_artifacts/continuous_scaler.pkl')\n",
    "scaled_continuous_features = common.load_and_transform_scaler(df_continuous, 'model_artifacts/continuous_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (8000, 768)\n",
      "Reduced shape: (8000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original shape:\", ad_copy_embeddings.shape)\n",
    "print(\"Reduced shape:\", reduced_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Categorical, Continuous, and Embedding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37254902 0.17820526 0.3006587  ... 0.         2.         2.        ]\n",
      " [0.62745098 0.08605793 0.64060158 ... 3.         0.         0.        ]\n",
      " [0.03921569 0.08068623 0.55672324 ... 4.         1.         2.        ]\n",
      " ...\n",
      " [0.41176471 0.20290479 0.3813765  ... 4.         0.         2.        ]\n",
      " [0.21568627 0.18243169 0.10348502 ... 2.         3.         3.        ]\n",
      " [0.33333333 0.05982505 0.25631136 ... 1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "combined_features = np.hstack((scaled_continuous_features, scaled_ad_copy_embeddings, encoded_categorical_data))\n",
    "print(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_continuous_array: (8000, 2)\n",
      "Shape of embeddings_array: (8000, 30)\n",
      "Shape of combined_features: (8000, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of df_continuous_array:\", scaled_continuous_features.shape)\n",
    "print(\"Shape of embeddings_array:\", scaled_ad_copy_embeddings.shape)\n",
    "print(\"Shape of combined_features:\", combined_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row: [59 4.226534379409992]\n",
      "Combined array row: [0.11764706 0.37470094]\n",
      "[[3.72549020e-01 1.78205264e-01 3.00658703e-01 2.98680067e-01\n",
      "  2.69747376e-02 4.18405771e-01 0.00000000e+00 9.99999166e-01\n",
      "  7.18948305e-01 4.18236554e-02 2.68972397e-01 2.67280817e-01\n",
      "  2.67930508e-01 1.58325344e-01 5.46252251e-01 2.01159701e-01\n",
      "  5.19606650e-01 5.95319510e-01 5.67930698e-01 2.35751793e-01\n",
      "  4.50963259e-01 5.48844278e-01 2.91155517e-01 1.01641893e-01\n",
      "  4.90868390e-01 2.11466908e-01 1.56110346e-01 2.52998233e-01\n",
      "  6.54318511e-01 5.68582475e-01 6.40719652e-01 3.28935564e-01\n",
      "  0.00000000e+00 2.00000000e+00 2.00000000e+00]\n",
      " [6.27450980e-01 8.60579296e-02 6.40601575e-01 9.44982767e-02\n",
      "  1.13628253e-01 3.04740667e-01 1.87279791e-01 7.49659657e-01\n",
      "  2.83815384e-01 0.00000000e+00 4.21317071e-01 3.66452634e-01\n",
      "  9.71843004e-02 2.95820594e-01 4.41854030e-01 5.70750535e-02\n",
      "  6.51248574e-01 7.06772327e-01 2.63707101e-01 3.18650544e-01\n",
      "  2.80897141e-01 4.63493943e-01 1.21021986e-01 5.50440133e-01\n",
      "  2.29978547e-01 4.29446459e-01 5.11782646e-01 4.51119781e-01\n",
      "  3.83792192e-01 4.63802606e-01 4.95966017e-01 1.34670138e-01\n",
      "  3.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.92156863e-02 8.06862285e-02 5.56723237e-01 8.16872597e-01\n",
      "  1.66754782e-01 6.94078088e-01 5.96028805e-01 4.89284754e-01\n",
      "  5.93136430e-01 1.66541740e-01 3.48881245e-01 4.88418162e-01\n",
      "  3.74677151e-01 5.05873978e-01 6.17310464e-01 1.89550146e-01\n",
      "  3.88081610e-01 6.18392348e-01 7.42848814e-01 2.69824117e-01\n",
      "  6.55305386e-02 2.39832237e-01 7.36570120e-01 3.38538855e-01\n",
      "  4.69043106e-01 6.86904371e-01 2.87533581e-01 6.28356218e-01\n",
      "  5.27501285e-01 3.28959107e-01 2.24953532e-01 4.54199255e-01\n",
      "  4.00000000e+00 1.00000000e+00 2.00000000e+00]\n",
      " [2.94117647e-01 3.34150828e-02 5.84386706e-01 7.44029045e-01\n",
      "  7.18671322e-01 7.02482164e-01 3.27307492e-01 5.11521876e-01\n",
      "  7.23896980e-01 3.43015760e-01 5.36410749e-01 7.85869718e-01\n",
      "  3.10946167e-01 4.91492748e-01 2.17659771e-01 4.87476051e-01\n",
      "  1.81766242e-01 3.35056484e-01 4.33798730e-01 2.69937187e-01\n",
      "  4.71277058e-01 5.80629826e-01 3.37742388e-01 3.26957405e-01\n",
      "  4.88053024e-01 4.46382612e-01 5.61764777e-01 4.93245423e-01\n",
      "  2.64814496e-03 5.03360331e-01 6.42920613e-01 5.03588676e-01\n",
      "  3.00000000e+00 4.00000000e+00 0.00000000e+00]\n",
      " [5.88235294e-02 1.51824701e-01 5.37600636e-01 8.07677269e-01\n",
      "  4.59049940e-01 5.72554588e-01 4.68530118e-01 6.49379730e-01\n",
      "  4.73662764e-01 1.71043545e-01 4.15014863e-01 5.91939092e-01\n",
      "  3.51938158e-01 4.91335988e-01 3.74959409e-01 2.59804845e-01\n",
      "  2.13279754e-01 2.97840208e-01 6.13534987e-01 3.75800163e-01\n",
      "  1.63208991e-01 4.18437213e-01 4.68398511e-01 5.48930526e-01\n",
      "  6.17810607e-01 4.73955631e-01 6.70196354e-01 5.80808759e-01\n",
      "  4.68602002e-01 4.83597249e-01 8.18821430e-01 7.65094161e-01\n",
      "  3.00000000e+00 4.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Sample a row index to examine\n",
    "row_index = 5\n",
    "\n",
    "original_row = df.iloc[row_index][continuous_fields].values\n",
    "print(\"Original row:\", original_row)\n",
    "\n",
    "# Get the corresponding row from the combined array\n",
    "combined_row = combined_features[770, :len(continuous_fields)]\n",
    "print(\"Combined array row:\", combined_row)\n",
    "print(combined_features[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                user_id                                 ad_id  \\\n",
      "0  6461380c-f03c-432d-abf8-d4f9fb1f4e28  31516856-be90-4155-b681-87c31fa946b8   \n",
      "1  8d58fb8a-5656-408d-9bf0-330fee82bf12  407fe646-bb2d-4f25-a630-606450b83434   \n",
      "2  db166483-dd99-4420-8265-2981477a359d  7f400246-d613-4d91-89eb-fa3e301ea58b   \n",
      "3  a50ad23e-90ed-4845-b62f-c849263eeb90  16feceb3-178f-41b5-b057-9b765543f843   \n",
      "4  1d9d9266-5cd3-4b25-be2d-8fd74af7cd28  98736a79-cd33-490e-b18b-7f000b2796be   \n",
      "\n",
      "   age       location                                            ad_copy  \\\n",
      "0   37         Africa  Captivate the Crowd with your Yeet Charisma Co...   \n",
      "1   50  North America  Captivate the Crowd with Yeet Charisma Couture...   \n",
      "2   20  South America  Experience Cinematic Brilliance at Home with F...   \n",
      "3   33  North America  Efficiency Meets Elegance with Aurora Hybrid.B...   \n",
      "4   21  North America  Experience Unmatched Elegance with Serenity Lu...   \n",
      "\n",
      "  product_type  ad_clicked  ad_type  pages_visited_this_session  \\\n",
      "0     clothing           0  sidebar                           8   \n",
      "1       beauty           0   banner                           3   \n",
      "2        books           0  sidebar                           6   \n",
      "3         home           1   banner                           8   \n",
      "4         home           0   banner                           4   \n",
      "\n",
      "   site_visit_frequency  \n",
      "0              4.034782  \n",
      "1              1.971319  \n",
      "2              1.851031  \n",
      "3              0.792484  \n",
      "4              3.444040  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Target Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of ads clicked (ad_clicked = 1): 41.04%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of 'ad_clicked' equals 1\n",
    "percentage_clicked = (df['ad_clicked'].sum() / len(df)) * 100\n",
    "\n",
    "print(f\"Percentage of ads clicked (ad_clicked = 1): {percentage_clicked:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "                      # Number of continuous features + ad_copy_embeddings length\n",
    "num_dense_features = len(scaled_continuous_features[0]) + scaled_ad_copy_embeddings.shape[1]  \n",
    "cat_embedding_sizes = [len(label_encoders[col].classes_) for col in categorical_cols]\n",
    "\n",
    "model = DLRM_Net(num_dense_features=num_dense_features, cat_embedding_sizes=cat_embedding_sizes)\n",
    "model_save_path = 'model_artifacts/trained_model.pt'\n",
    "learning_rate = 0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(combined_features, dtype=torch.float32)\n",
    "y = torch.tensor(df['ad_clicked'].to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = DLRM_Dataset(X_train, y_train)\n",
    "val_dataset = DLRM_Dataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Training Loss: 0.6811, Validation Loss: 0.6870\n",
      "Epoch [2/500], Training Loss: 0.6766, Validation Loss: 0.6848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/500], Training Loss: 0.6795, Validation Loss: 0.6831\n",
      "Epoch [4/500], Training Loss: 0.6780, Validation Loss: 0.6817\n",
      "Epoch [5/500], Training Loss: 0.6762, Validation Loss: 0.6808\n",
      "Epoch [6/500], Training Loss: 0.6730, Validation Loss: 0.6801\n",
      "Epoch [7/500], Training Loss: 0.6740, Validation Loss: 0.6796\n",
      "Epoch [8/500], Training Loss: 0.6723, Validation Loss: 0.6782\n",
      "Epoch [9/500], Training Loss: 0.6720, Validation Loss: 0.6753\n",
      "Epoch [10/500], Training Loss: 0.6667, Validation Loss: 0.6694\n",
      "Epoch [11/500], Training Loss: 0.6590, Validation Loss: 0.6592\n",
      "Epoch [12/500], Training Loss: 0.6456, Validation Loss: 0.6450\n",
      "Epoch [13/500], Training Loss: 0.6309, Validation Loss: 0.6272\n",
      "Epoch [14/500], Training Loss: 0.6092, Validation Loss: 0.6018\n",
      "Epoch [15/500], Training Loss: 0.5757, Validation Loss: 0.5722\n",
      "Epoch [16/500], Training Loss: 0.5321, Validation Loss: 0.5413\n",
      "Epoch [17/500], Training Loss: 0.5082, Validation Loss: 0.5093\n",
      "Epoch [18/500], Training Loss: 0.4742, Validation Loss: 0.5009\n",
      "Epoch [19/500], Training Loss: 0.4693, Validation Loss: 0.4840\n",
      "Epoch [20/500], Training Loss: 0.4545, Validation Loss: 0.4740\n",
      "Epoch [21/500], Training Loss: 0.4362, Validation Loss: 0.4587\n",
      "Epoch [22/500], Training Loss: 0.4256, Validation Loss: 0.4460\n",
      "Epoch [23/500], Training Loss: 0.4126, Validation Loss: 0.4277\n",
      "Epoch [24/500], Training Loss: 0.3993, Validation Loss: 0.4104\n",
      "Epoch [25/500], Training Loss: 0.3803, Validation Loss: 0.3933\n",
      "Epoch [26/500], Training Loss: 0.3624, Validation Loss: 0.3783\n",
      "Epoch [27/500], Training Loss: 0.3564, Validation Loss: 0.3700\n",
      "Epoch [28/500], Training Loss: 0.3417, Validation Loss: 0.3596\n",
      "Epoch [29/500], Training Loss: 0.3312, Validation Loss: 0.3484\n",
      "Epoch [30/500], Training Loss: 0.3262, Validation Loss: 0.3401\n",
      "Epoch [31/500], Training Loss: 0.3233, Validation Loss: 0.3328\n",
      "Epoch [32/500], Training Loss: 0.3128, Validation Loss: 0.3269\n",
      "Epoch [33/500], Training Loss: 0.3137, Validation Loss: 0.3217\n",
      "Epoch [34/500], Training Loss: 0.3024, Validation Loss: 0.3177\n",
      "Epoch [35/500], Training Loss: 0.3001, Validation Loss: 0.3141\n",
      "Epoch [36/500], Training Loss: 0.2942, Validation Loss: 0.3115\n",
      "Epoch [37/500], Training Loss: 0.2963, Validation Loss: 0.3066\n",
      "Epoch [38/500], Training Loss: 0.2932, Validation Loss: 0.3041\n",
      "Epoch [39/500], Training Loss: 0.2848, Validation Loss: 0.3031\n",
      "Epoch [40/500], Training Loss: 0.2907, Validation Loss: 0.3002\n",
      "Epoch [41/500], Training Loss: 0.2831, Validation Loss: 0.2993\n",
      "Epoch [42/500], Training Loss: 0.2808, Validation Loss: 0.2940\n",
      "Epoch [43/500], Training Loss: 0.2817, Validation Loss: 0.2915\n",
      "Epoch [44/500], Training Loss: 0.2794, Validation Loss: 0.2893\n",
      "Epoch [45/500], Training Loss: 0.2803, Validation Loss: 0.2875\n",
      "Epoch [46/500], Training Loss: 0.2759, Validation Loss: 0.2868\n",
      "Epoch [47/500], Training Loss: 0.2700, Validation Loss: 0.2912\n",
      "Epoch [48/500], Training Loss: 0.2753, Validation Loss: 0.2820\n",
      "Epoch [49/500], Training Loss: 0.2650, Validation Loss: 0.2801\n",
      "Epoch [50/500], Training Loss: 0.2688, Validation Loss: 0.2794\n",
      "Epoch [51/500], Training Loss: 0.2629, Validation Loss: 0.2779\n",
      "Epoch [52/500], Training Loss: 0.2614, Validation Loss: 0.2734\n",
      "Epoch [53/500], Training Loss: 0.2608, Validation Loss: 0.2735\n",
      "Epoch [54/500], Training Loss: 0.2574, Validation Loss: 0.2704\n",
      "Epoch [55/500], Training Loss: 0.2608, Validation Loss: 0.2684\n",
      "Epoch [56/500], Training Loss: 0.2572, Validation Loss: 0.2649\n",
      "Epoch [57/500], Training Loss: 0.2556, Validation Loss: 0.2640\n",
      "Epoch [58/500], Training Loss: 0.2513, Validation Loss: 0.2612\n",
      "Epoch [59/500], Training Loss: 0.2414, Validation Loss: 0.2559\n",
      "Epoch [60/500], Training Loss: 0.2395, Validation Loss: 0.2537\n",
      "Epoch [61/500], Training Loss: 0.2395, Validation Loss: 0.2511\n",
      "Epoch [62/500], Training Loss: 0.2355, Validation Loss: 0.2473\n",
      "Epoch [63/500], Training Loss: 0.2302, Validation Loss: 0.2390\n",
      "Epoch [64/500], Training Loss: 0.2295, Validation Loss: 0.2364\n",
      "Epoch [65/500], Training Loss: 0.2295, Validation Loss: 0.2333\n",
      "Epoch [66/500], Training Loss: 0.2273, Validation Loss: 0.2297\n",
      "Epoch [67/500], Training Loss: 0.2168, Validation Loss: 0.2291\n",
      "Epoch [68/500], Training Loss: 0.2160, Validation Loss: 0.2242\n",
      "Epoch [69/500], Training Loss: 0.2124, Validation Loss: 0.2198\n",
      "Epoch [70/500], Training Loss: 0.2092, Validation Loss: 0.2182\n",
      "Epoch [71/500], Training Loss: 0.2094, Validation Loss: 0.2152\n",
      "Epoch [72/500], Training Loss: 0.2039, Validation Loss: 0.2147\n",
      "Epoch [73/500], Training Loss: 0.2086, Validation Loss: 0.2161\n",
      "Epoch [74/500], Training Loss: 0.2023, Validation Loss: 0.2110\n",
      "Epoch [75/500], Training Loss: 0.1980, Validation Loss: 0.2096\n",
      "Epoch [76/500], Training Loss: 0.1958, Validation Loss: 0.2067\n",
      "Epoch [77/500], Training Loss: 0.1955, Validation Loss: 0.2021\n",
      "Epoch [78/500], Training Loss: 0.1907, Validation Loss: 0.2001\n",
      "Epoch [79/500], Training Loss: 0.2029, Validation Loss: 0.2016\n",
      "Epoch [80/500], Training Loss: 0.1935, Validation Loss: 0.1969\n",
      "Epoch [81/500], Training Loss: 0.1889, Validation Loss: 0.1992\n",
      "Epoch [82/500], Training Loss: 0.1837, Validation Loss: 0.1939\n",
      "Epoch [83/500], Training Loss: 0.1843, Validation Loss: 0.1941\n",
      "Epoch [84/500], Training Loss: 0.1794, Validation Loss: 0.1902\n",
      "Epoch [85/500], Training Loss: 0.1785, Validation Loss: 0.1896\n",
      "Epoch [86/500], Training Loss: 0.1784, Validation Loss: 0.1880\n",
      "Epoch [87/500], Training Loss: 0.1782, Validation Loss: 0.1869\n",
      "Epoch [88/500], Training Loss: 0.1749, Validation Loss: 0.1855\n",
      "Epoch [89/500], Training Loss: 0.1747, Validation Loss: 0.1853\n",
      "Epoch [90/500], Training Loss: 0.1785, Validation Loss: 0.1947\n",
      "Epoch [91/500], Training Loss: 0.1837, Validation Loss: 0.1858\n",
      "Epoch [92/500], Training Loss: 0.1816, Validation Loss: 0.1938\n",
      "Epoch [93/500], Training Loss: 0.1767, Validation Loss: 0.1811\n",
      "Epoch [94/500], Training Loss: 0.1709, Validation Loss: 0.1774\n",
      "Epoch [95/500], Training Loss: 0.1679, Validation Loss: 0.1770\n",
      "Epoch [96/500], Training Loss: 0.1626, Validation Loss: 0.1734\n",
      "Epoch [97/500], Training Loss: 0.1733, Validation Loss: 0.1729\n",
      "Epoch [98/500], Training Loss: 0.1601, Validation Loss: 0.1720\n",
      "Epoch [99/500], Training Loss: 0.1590, Validation Loss: 0.1687\n",
      "Epoch [100/500], Training Loss: 0.1593, Validation Loss: 0.1683\n",
      "Epoch [101/500], Training Loss: 0.1611, Validation Loss: 0.1650\n",
      "Epoch [102/500], Training Loss: 0.1593, Validation Loss: 0.1660\n",
      "Epoch [103/500], Training Loss: 0.1525, Validation Loss: 0.1641\n",
      "Epoch [104/500], Training Loss: 0.1531, Validation Loss: 0.1611\n",
      "Epoch [105/500], Training Loss: 0.1522, Validation Loss: 0.1714\n",
      "Epoch [106/500], Training Loss: 0.1595, Validation Loss: 0.1690\n",
      "Epoch [107/500], Training Loss: 0.1567, Validation Loss: 0.1630\n",
      "Epoch [108/500], Training Loss: 0.1574, Validation Loss: 0.1565\n",
      "Epoch [109/500], Training Loss: 0.1495, Validation Loss: 0.1533\n",
      "Epoch [110/500], Training Loss: 0.1404, Validation Loss: 0.1506\n",
      "Epoch [111/500], Training Loss: 0.1404, Validation Loss: 0.1468\n",
      "Epoch [112/500], Training Loss: 0.1398, Validation Loss: 0.1524\n",
      "Epoch [113/500], Training Loss: 0.1406, Validation Loss: 0.1431\n",
      "Epoch [114/500], Training Loss: 0.1379, Validation Loss: 0.1430\n",
      "Epoch [115/500], Training Loss: 0.1319, Validation Loss: 0.1413\n",
      "Epoch [116/500], Training Loss: 0.1320, Validation Loss: 0.1381\n",
      "Epoch [117/500], Training Loss: 0.1323, Validation Loss: 0.1361\n",
      "Epoch [118/500], Training Loss: 0.1350, Validation Loss: 0.1362\n",
      "Epoch [119/500], Training Loss: 0.1309, Validation Loss: 0.1399\n",
      "Epoch [120/500], Training Loss: 0.1351, Validation Loss: 0.1318\n",
      "Epoch [121/500], Training Loss: 0.1251, Validation Loss: 0.1293\n",
      "Epoch [122/500], Training Loss: 0.1244, Validation Loss: 0.1272\n",
      "Epoch [123/500], Training Loss: 0.1164, Validation Loss: 0.1262\n",
      "Epoch [124/500], Training Loss: 0.1202, Validation Loss: 0.1316\n",
      "Epoch [125/500], Training Loss: 0.1178, Validation Loss: 0.1478\n",
      "Epoch [126/500], Training Loss: 0.1297, Validation Loss: 0.1433\n",
      "Epoch [127/500], Training Loss: 0.1165, Validation Loss: 0.1374\n",
      "Epoch [128/500], Training Loss: 0.1180, Validation Loss: 0.1308\n",
      "Epoch [129/500], Training Loss: 0.1130, Validation Loss: 0.1356\n",
      "Epoch [130/500], Training Loss: 0.1205, Validation Loss: 0.1272\n",
      "Epoch [131/500], Training Loss: 0.1111, Validation Loss: 0.1151\n",
      "Epoch [132/500], Training Loss: 0.1052, Validation Loss: 0.1199\n",
      "Epoch [133/500], Training Loss: 0.1081, Validation Loss: 0.1103\n",
      "Epoch [134/500], Training Loss: 0.1067, Validation Loss: 0.1104\n",
      "Epoch [135/500], Training Loss: 0.1060, Validation Loss: 0.1090\n",
      "Epoch [136/500], Training Loss: 0.1029, Validation Loss: 0.1072\n",
      "Epoch [137/500], Training Loss: 0.1015, Validation Loss: 0.1112\n",
      "Epoch [138/500], Training Loss: 0.0992, Validation Loss: 0.1093\n",
      "Epoch [139/500], Training Loss: 0.1014, Validation Loss: 0.1050\n",
      "Epoch [140/500], Training Loss: 0.0997, Validation Loss: 0.1167\n",
      "Epoch [141/500], Training Loss: 0.1061, Validation Loss: 0.1238\n",
      "Epoch [142/500], Training Loss: 0.1042, Validation Loss: 0.1099\n",
      "Epoch [143/500], Training Loss: 0.1053, Validation Loss: 0.1027\n",
      "Epoch [144/500], Training Loss: 0.0961, Validation Loss: 0.1138\n",
      "Epoch [145/500], Training Loss: 0.0993, Validation Loss: 0.1109\n",
      "Epoch [146/500], Training Loss: 0.0998, Validation Loss: 0.1139\n",
      "Epoch [147/500], Training Loss: 0.0977, Validation Loss: 0.1011\n",
      "Epoch [148/500], Training Loss: 0.0953, Validation Loss: 0.1082\n",
      "Epoch [149/500], Training Loss: 0.0955, Validation Loss: 0.0996\n",
      "Epoch [150/500], Training Loss: 0.0920, Validation Loss: 0.1033\n",
      "Epoch [151/500], Training Loss: 0.0994, Validation Loss: 0.0978\n",
      "Epoch [152/500], Training Loss: 0.0949, Validation Loss: 0.0964\n",
      "Epoch [153/500], Training Loss: 0.0886, Validation Loss: 0.0948\n",
      "Epoch [154/500], Training Loss: 0.0935, Validation Loss: 0.1012\n",
      "Epoch [155/500], Training Loss: 0.0921, Validation Loss: 0.0946\n",
      "Epoch [156/500], Training Loss: 0.0901, Validation Loss: 0.0996\n",
      "Epoch [157/500], Training Loss: 0.0867, Validation Loss: 0.0954\n",
      "Epoch [158/500], Training Loss: 0.0874, Validation Loss: 0.0961\n",
      "Epoch [159/500], Training Loss: 0.0878, Validation Loss: 0.1000\n",
      "Epoch [160/500], Training Loss: 0.0896, Validation Loss: 0.1013\n",
      "Epoch [161/500], Training Loss: 0.0889, Validation Loss: 0.1014\n",
      "Epoch [162/500], Training Loss: 0.0877, Validation Loss: 0.0939\n",
      "Epoch [163/500], Training Loss: 0.0881, Validation Loss: 0.0963\n",
      "Epoch [164/500], Training Loss: 0.0862, Validation Loss: 0.0943\n",
      "Epoch [165/500], Training Loss: 0.0924, Validation Loss: 0.0955\n",
      "Epoch [166/500], Training Loss: 0.0874, Validation Loss: 0.0953\n",
      "Epoch [167/500], Training Loss: 0.0817, Validation Loss: 0.0968\n",
      "Epoch [168/500], Training Loss: 0.0828, Validation Loss: 0.0953\n",
      "Epoch [169/500], Training Loss: 0.0821, Validation Loss: 0.0919\n",
      "Epoch [170/500], Training Loss: 0.0834, Validation Loss: 0.0927\n",
      "Epoch [171/500], Training Loss: 0.0792, Validation Loss: 0.0914\n",
      "Epoch [172/500], Training Loss: 0.0821, Validation Loss: 0.0965\n",
      "Epoch [173/500], Training Loss: 0.0840, Validation Loss: 0.1059\n",
      "Epoch [174/500], Training Loss: 0.0836, Validation Loss: 0.0894\n",
      "Epoch [175/500], Training Loss: 0.0834, Validation Loss: 0.0893\n",
      "Epoch [176/500], Training Loss: 0.0791, Validation Loss: 0.1012\n",
      "Epoch [177/500], Training Loss: 0.0874, Validation Loss: 0.0912\n",
      "Epoch [178/500], Training Loss: 0.0796, Validation Loss: 0.0882\n",
      "Epoch [179/500], Training Loss: 0.0811, Validation Loss: 0.0995\n",
      "Epoch [180/500], Training Loss: 0.0853, Validation Loss: 0.1041\n",
      "Epoch [181/500], Training Loss: 0.0855, Validation Loss: 0.0963\n",
      "Epoch [182/500], Training Loss: 0.0836, Validation Loss: 0.0888\n",
      "Epoch [183/500], Training Loss: 0.0801, Validation Loss: 0.0911\n",
      "Epoch [184/500], Training Loss: 0.0841, Validation Loss: 0.1045\n",
      "Epoch [185/500], Training Loss: 0.0852, Validation Loss: 0.0949\n",
      "Epoch [186/500], Training Loss: 0.0818, Validation Loss: 0.0953\n",
      "Epoch [187/500], Training Loss: 0.0835, Validation Loss: 0.0891\n",
      "Epoch [188/500], Training Loss: 0.0835, Validation Loss: 0.1002\n",
      "Epoch [189/500], Training Loss: 0.0803, Validation Loss: 0.0884\n",
      "Epoch [190/500], Training Loss: 0.0763, Validation Loss: 0.0847\n",
      "Epoch [191/500], Training Loss: 0.0814, Validation Loss: 0.0974\n",
      "Epoch [192/500], Training Loss: 0.0760, Validation Loss: 0.0837\n",
      "Epoch [193/500], Training Loss: 0.0836, Validation Loss: 0.0843\n",
      "Epoch [194/500], Training Loss: 0.0742, Validation Loss: 0.0851\n",
      "Epoch [195/500], Training Loss: 0.0779, Validation Loss: 0.0896\n",
      "Epoch [196/500], Training Loss: 0.0763, Validation Loss: 0.0865\n",
      "Epoch [197/500], Training Loss: 0.0787, Validation Loss: 0.0891\n",
      "Epoch [198/500], Training Loss: 0.0734, Validation Loss: 0.0949\n",
      "Epoch [199/500], Training Loss: 0.0772, Validation Loss: 0.0821\n",
      "Epoch [200/500], Training Loss: 0.0737, Validation Loss: 0.0847\n",
      "Epoch [201/500], Training Loss: 0.0825, Validation Loss: 0.0909\n",
      "Epoch [202/500], Training Loss: 0.0810, Validation Loss: 0.0914\n",
      "Epoch [203/500], Training Loss: 0.0795, Validation Loss: 0.0858\n",
      "Epoch [204/500], Training Loss: 0.0721, Validation Loss: 0.0800\n",
      "Epoch [205/500], Training Loss: 0.0784, Validation Loss: 0.0794\n",
      "Epoch [206/500], Training Loss: 0.0808, Validation Loss: 0.0947\n",
      "Epoch [207/500], Training Loss: 0.0843, Validation Loss: 0.0849\n",
      "Epoch [208/500], Training Loss: 0.0828, Validation Loss: 0.0875\n",
      "Epoch [209/500], Training Loss: 0.0782, Validation Loss: 0.0845\n",
      "Epoch [210/500], Training Loss: 0.0764, Validation Loss: 0.0903\n",
      "Epoch [211/500], Training Loss: 0.0745, Validation Loss: 0.0832\n",
      "Epoch [212/500], Training Loss: 0.0705, Validation Loss: 0.0828\n",
      "Epoch [213/500], Training Loss: 0.0772, Validation Loss: 0.0889\n",
      "Epoch [214/500], Training Loss: 0.0760, Validation Loss: 0.0817\n",
      "Epoch [215/500], Training Loss: 0.0695, Validation Loss: 0.0799\n",
      "Epoch [216/500], Training Loss: 0.0771, Validation Loss: 0.0849\n",
      "Epoch [217/500], Training Loss: 0.0739, Validation Loss: 0.0825\n",
      "Epoch [218/500], Training Loss: 0.0739, Validation Loss: 0.0783\n",
      "Epoch [219/500], Training Loss: 0.0715, Validation Loss: 0.0814\n",
      "Epoch [220/500], Training Loss: 0.0693, Validation Loss: 0.0806\n",
      "Epoch [221/500], Training Loss: 0.0706, Validation Loss: 0.0791\n",
      "Epoch [222/500], Training Loss: 0.0693, Validation Loss: 0.0885\n",
      "Epoch [223/500], Training Loss: 0.0738, Validation Loss: 0.0800\n",
      "Epoch [224/500], Training Loss: 0.0681, Validation Loss: 0.0784\n",
      "Epoch [225/500], Training Loss: 0.0694, Validation Loss: 0.0789\n",
      "Epoch [226/500], Training Loss: 0.0721, Validation Loss: 0.0938\n",
      "Epoch [227/500], Training Loss: 0.0727, Validation Loss: 0.0832\n",
      "Epoch [228/500], Training Loss: 0.0741, Validation Loss: 0.0774\n",
      "Epoch [229/500], Training Loss: 0.0686, Validation Loss: 0.0805\n",
      "Epoch [230/500], Training Loss: 0.0687, Validation Loss: 0.0803\n",
      "Epoch [231/500], Training Loss: 0.0678, Validation Loss: 0.0786\n",
      "Epoch [232/500], Training Loss: 0.0731, Validation Loss: 0.0767\n",
      "Epoch [233/500], Training Loss: 0.0724, Validation Loss: 0.0928\n",
      "Epoch [234/500], Training Loss: 0.0773, Validation Loss: 0.0835\n",
      "Epoch [235/500], Training Loss: 0.0745, Validation Loss: 0.0869\n",
      "Epoch [236/500], Training Loss: 0.0755, Validation Loss: 0.1072\n",
      "Epoch [237/500], Training Loss: 0.0800, Validation Loss: 0.0962\n",
      "Epoch [238/500], Training Loss: 0.0796, Validation Loss: 0.1062\n",
      "Epoch [239/500], Training Loss: 0.0851, Validation Loss: 0.0893\n",
      "Epoch [240/500], Training Loss: 0.0866, Validation Loss: 0.0851\n",
      "Epoch [241/500], Training Loss: 0.0770, Validation Loss: 0.0796\n",
      "Epoch [242/500], Training Loss: 0.0725, Validation Loss: 0.0761\n",
      "Epoch [243/500], Training Loss: 0.0687, Validation Loss: 0.0817\n",
      "Epoch [244/500], Training Loss: 0.0732, Validation Loss: 0.0870\n",
      "Epoch [245/500], Training Loss: 0.0699, Validation Loss: 0.0808\n",
      "Epoch [246/500], Training Loss: 0.0723, Validation Loss: 0.0796\n",
      "Epoch [247/500], Training Loss: 0.0680, Validation Loss: 0.0838\n",
      "Epoch [248/500], Training Loss: 0.0711, Validation Loss: 0.0836\n",
      "Epoch [249/500], Training Loss: 0.0771, Validation Loss: 0.0993\n",
      "Epoch [250/500], Training Loss: 0.0784, Validation Loss: 0.0822\n",
      "Epoch [251/500], Training Loss: 0.0736, Validation Loss: 0.0894\n",
      "Epoch [252/500], Training Loss: 0.0752, Validation Loss: 0.0844\n",
      "Epoch [253/500], Training Loss: 0.0775, Validation Loss: 0.0894\n",
      "Epoch [254/500], Training Loss: 0.0756, Validation Loss: 0.0880\n",
      "Epoch [255/500], Training Loss: 0.0729, Validation Loss: 0.0918\n",
      "Epoch [256/500], Training Loss: 0.0739, Validation Loss: 0.0915\n",
      "Epoch [257/500], Training Loss: 0.0732, Validation Loss: 0.0839\n",
      "Epoch [258/500], Training Loss: 0.0699, Validation Loss: 0.0853\n",
      "Epoch [259/500], Training Loss: 0.0715, Validation Loss: 0.0803\n",
      "Epoch [260/500], Training Loss: 0.0704, Validation Loss: 0.0848\n",
      "Epoch [261/500], Training Loss: 0.0696, Validation Loss: 0.0934\n",
      "Epoch [262/500], Training Loss: 0.0757, Validation Loss: 0.0794\n",
      "Epoch [263/500], Training Loss: 0.0689, Validation Loss: 0.0783\n",
      "Epoch [264/500], Training Loss: 0.0634, Validation Loss: 0.0845\n",
      "Epoch [265/500], Training Loss: 0.0643, Validation Loss: 0.0781\n",
      "Epoch [266/500], Training Loss: 0.0679, Validation Loss: 0.0781\n",
      "Epoch [267/500], Training Loss: 0.0758, Validation Loss: 0.0780\n",
      "Epoch [268/500], Training Loss: 0.0663, Validation Loss: 0.0802\n",
      "Epoch [269/500], Training Loss: 0.0708, Validation Loss: 0.0897\n",
      "Epoch [270/500], Training Loss: 0.0743, Validation Loss: 0.0869\n",
      "Epoch [271/500], Training Loss: 0.0797, Validation Loss: 0.0786\n",
      "Epoch [272/500], Training Loss: 0.0682, Validation Loss: 0.0822\n",
      "Epoch [273/500], Training Loss: 0.0688, Validation Loss: 0.0759\n",
      "Epoch [274/500], Training Loss: 0.0647, Validation Loss: 0.0774\n",
      "Epoch [275/500], Training Loss: 0.0644, Validation Loss: 0.0823\n",
      "Epoch [276/500], Training Loss: 0.0678, Validation Loss: 0.0817\n",
      "Epoch [277/500], Training Loss: 0.0678, Validation Loss: 0.0835\n",
      "Epoch [278/500], Training Loss: 0.0702, Validation Loss: 0.0749\n",
      "Epoch [279/500], Training Loss: 0.0694, Validation Loss: 0.0797\n",
      "Epoch [280/500], Training Loss: 0.0639, Validation Loss: 0.0758\n",
      "Epoch [281/500], Training Loss: 0.0613, Validation Loss: 0.0785\n",
      "Epoch [282/500], Training Loss: 0.0664, Validation Loss: 0.0771\n",
      "Epoch [283/500], Training Loss: 0.0654, Validation Loss: 0.0779\n",
      "Epoch [284/500], Training Loss: 0.0647, Validation Loss: 0.0769\n",
      "Epoch [285/500], Training Loss: 0.0664, Validation Loss: 0.0815\n",
      "Epoch [286/500], Training Loss: 0.0654, Validation Loss: 0.0766\n",
      "Epoch [287/500], Training Loss: 0.0623, Validation Loss: 0.0796\n",
      "Epoch [288/500], Training Loss: 0.0632, Validation Loss: 0.0784\n",
      "Epoch [289/500], Training Loss: 0.0694, Validation Loss: 0.0781\n",
      "Epoch [290/500], Training Loss: 0.0649, Validation Loss: 0.0780\n",
      "Epoch [291/500], Training Loss: 0.0635, Validation Loss: 0.0753\n",
      "Epoch [292/500], Training Loss: 0.0704, Validation Loss: 0.0849\n",
      "Epoch [293/500], Training Loss: 0.0707, Validation Loss: 0.0844\n",
      "Epoch [294/500], Training Loss: 0.0724, Validation Loss: 0.0857\n",
      "Epoch [295/500], Training Loss: 0.0817, Validation Loss: 0.0799\n",
      "Epoch [296/500], Training Loss: 0.0734, Validation Loss: 0.0849\n",
      "Epoch [297/500], Training Loss: 0.0706, Validation Loss: 0.0938\n",
      "Epoch [298/500], Training Loss: 0.0677, Validation Loss: 0.0830\n",
      "Epoch [299/500], Training Loss: 0.0751, Validation Loss: 0.0769\n",
      "Epoch [300/500], Training Loss: 0.0668, Validation Loss: 0.0834\n",
      "Epoch [301/500], Training Loss: 0.0758, Validation Loss: 0.0802\n",
      "Epoch [302/500], Training Loss: 0.0754, Validation Loss: 0.0774\n",
      "Epoch [303/500], Training Loss: 0.0699, Validation Loss: 0.0798\n",
      "Epoch [304/500], Training Loss: 0.0665, Validation Loss: 0.0809\n",
      "Epoch [305/500], Training Loss: 0.0670, Validation Loss: 0.0830\n",
      "Epoch [306/500], Training Loss: 0.0635, Validation Loss: 0.0785\n",
      "Epoch [307/500], Training Loss: 0.0604, Validation Loss: 0.0874\n",
      "Epoch [308/500], Training Loss: 0.0661, Validation Loss: 0.0911\n",
      "Epoch [309/500], Training Loss: 0.0706, Validation Loss: 0.1059\n",
      "Epoch [310/500], Training Loss: 0.0740, Validation Loss: 0.0974\n",
      "Epoch [311/500], Training Loss: 0.0725, Validation Loss: 0.1061\n",
      "Epoch [312/500], Training Loss: 0.0699, Validation Loss: 0.0869\n",
      "Epoch [313/500], Training Loss: 0.0655, Validation Loss: 0.0781\n",
      "Epoch [314/500], Training Loss: 0.0626, Validation Loss: 0.0769\n",
      "Epoch [315/500], Training Loss: 0.0620, Validation Loss: 0.0810\n",
      "Epoch [316/500], Training Loss: 0.0677, Validation Loss: 0.0812\n",
      "Epoch [317/500], Training Loss: 0.0685, Validation Loss: 0.0761\n",
      "Epoch [318/500], Training Loss: 0.0616, Validation Loss: 0.0804\n",
      "Epoch [319/500], Training Loss: 0.0637, Validation Loss: 0.0778\n",
      "Epoch [320/500], Training Loss: 0.0639, Validation Loss: 0.0757\n",
      "Epoch [321/500], Training Loss: 0.0666, Validation Loss: 0.0780\n",
      "Epoch [322/500], Training Loss: 0.0711, Validation Loss: 0.0847\n",
      "Epoch [323/500], Training Loss: 0.0650, Validation Loss: 0.0865\n",
      "Epoch [324/500], Training Loss: 0.0645, Validation Loss: 0.0771\n",
      "Epoch [325/500], Training Loss: 0.0708, Validation Loss: 0.0948\n",
      "Epoch [326/500], Training Loss: 0.0690, Validation Loss: 0.0821\n",
      "Epoch [327/500], Training Loss: 0.0606, Validation Loss: 0.0804\n",
      "Epoch [328/500], Training Loss: 0.0645, Validation Loss: 0.0762\n",
      "Epoch [329/500], Training Loss: 0.0623, Validation Loss: 0.0802\n",
      "Epoch [330/500], Training Loss: 0.0660, Validation Loss: 0.0815\n",
      "Epoch [331/500], Training Loss: 0.0638, Validation Loss: 0.0756\n",
      "Epoch [332/500], Training Loss: 0.0677, Validation Loss: 0.0863\n",
      "Epoch [333/500], Training Loss: 0.0686, Validation Loss: 0.1005\n",
      "Epoch [334/500], Training Loss: 0.0751, Validation Loss: 0.1174\n",
      "Epoch [335/500], Training Loss: 0.0857, Validation Loss: 0.0989\n",
      "Epoch [336/500], Training Loss: 0.0757, Validation Loss: 0.0903\n",
      "Epoch [337/500], Training Loss: 0.0669, Validation Loss: 0.0770\n",
      "Epoch [338/500], Training Loss: 0.0645, Validation Loss: 0.0878\n",
      "Epoch [339/500], Training Loss: 0.0701, Validation Loss: 0.0901\n",
      "Epoch [340/500], Training Loss: 0.0715, Validation Loss: 0.0874\n",
      "Epoch [341/500], Training Loss: 0.0649, Validation Loss: 0.0795\n",
      "Epoch [342/500], Training Loss: 0.0663, Validation Loss: 0.0803\n",
      "Epoch [343/500], Training Loss: 0.0704, Validation Loss: 0.0900\n",
      "Epoch [344/500], Training Loss: 0.0687, Validation Loss: 0.0767\n",
      "Epoch [345/500], Training Loss: 0.0600, Validation Loss: 0.0878\n",
      "Epoch [346/500], Training Loss: 0.0656, Validation Loss: 0.0824\n",
      "Epoch [347/500], Training Loss: 0.0657, Validation Loss: 0.0781\n",
      "Epoch [348/500], Training Loss: 0.0673, Validation Loss: 0.0745\n",
      "Epoch [349/500], Training Loss: 0.0623, Validation Loss: 0.0795\n",
      "Epoch [350/500], Training Loss: 0.0604, Validation Loss: 0.1142\n",
      "Epoch [351/500], Training Loss: 0.0754, Validation Loss: 0.0876\n",
      "Epoch [352/500], Training Loss: 0.0696, Validation Loss: 0.0768\n",
      "Epoch [353/500], Training Loss: 0.0623, Validation Loss: 0.0973\n",
      "Epoch [354/500], Training Loss: 0.0710, Validation Loss: 0.1005\n",
      "Epoch [355/500], Training Loss: 0.0738, Validation Loss: 0.0824\n",
      "Epoch [356/500], Training Loss: 0.0638, Validation Loss: 0.0837\n",
      "Epoch [357/500], Training Loss: 0.0657, Validation Loss: 0.0792\n",
      "Epoch [358/500], Training Loss: 0.0663, Validation Loss: 0.0882\n",
      "Epoch [359/500], Training Loss: 0.0623, Validation Loss: 0.0885\n",
      "Epoch [360/500], Training Loss: 0.0705, Validation Loss: 0.0901\n",
      "Epoch [361/500], Training Loss: 0.0678, Validation Loss: 0.0935\n",
      "Epoch [362/500], Training Loss: 0.0609, Validation Loss: 0.0775\n",
      "Epoch [363/500], Training Loss: 0.0633, Validation Loss: 0.0877\n",
      "Epoch [364/500], Training Loss: 0.0632, Validation Loss: 0.0871\n",
      "Epoch [365/500], Training Loss: 0.0679, Validation Loss: 0.0950\n",
      "Epoch [366/500], Training Loss: 0.0674, Validation Loss: 0.0756\n",
      "Epoch [367/500], Training Loss: 0.0592, Validation Loss: 0.0789\n",
      "Epoch [368/500], Training Loss: 0.0602, Validation Loss: 0.0753\n",
      "Epoch [369/500], Training Loss: 0.0624, Validation Loss: 0.0766\n",
      "Epoch [370/500], Training Loss: 0.0598, Validation Loss: 0.0846\n",
      "Epoch [371/500], Training Loss: 0.0689, Validation Loss: 0.1056\n",
      "Epoch [372/500], Training Loss: 0.0770, Validation Loss: 0.1044\n",
      "Epoch [373/500], Training Loss: 0.0807, Validation Loss: 0.0883\n",
      "Epoch [374/500], Training Loss: 0.0629, Validation Loss: 0.0868\n",
      "Epoch [375/500], Training Loss: 0.0676, Validation Loss: 0.0831\n",
      "Epoch [376/500], Training Loss: 0.0632, Validation Loss: 0.0763\n",
      "Epoch [377/500], Training Loss: 0.0575, Validation Loss: 0.0805\n",
      "Epoch [378/500], Training Loss: 0.0626, Validation Loss: 0.0771\n",
      "Epoch [379/500], Training Loss: 0.0710, Validation Loss: 0.0840\n",
      "Epoch [380/500], Training Loss: 0.0697, Validation Loss: 0.0778\n",
      "Epoch [381/500], Training Loss: 0.0663, Validation Loss: 0.0882\n",
      "Epoch [382/500], Training Loss: 0.0667, Validation Loss: 0.1098\n",
      "Epoch [383/500], Training Loss: 0.0719, Validation Loss: 0.0806\n",
      "Epoch [384/500], Training Loss: 0.0627, Validation Loss: 0.0833\n",
      "Epoch [385/500], Training Loss: 0.0609, Validation Loss: 0.0765\n",
      "Epoch [386/500], Training Loss: 0.0598, Validation Loss: 0.0769\n",
      "Epoch [387/500], Training Loss: 0.0650, Validation Loss: 0.0789\n",
      "Epoch [388/500], Training Loss: 0.0602, Validation Loss: 0.0764\n",
      "Epoch [389/500], Training Loss: 0.0619, Validation Loss: 0.0760\n",
      "Epoch [390/500], Training Loss: 0.0597, Validation Loss: 0.0756\n",
      "Epoch [391/500], Training Loss: 0.0596, Validation Loss: 0.0764\n",
      "Epoch [392/500], Training Loss: 0.0610, Validation Loss: 0.0767\n",
      "Epoch [393/500], Training Loss: 0.0569, Validation Loss: 0.0784\n",
      "Epoch [394/500], Training Loss: 0.0605, Validation Loss: 0.0813\n",
      "Epoch [395/500], Training Loss: 0.0591, Validation Loss: 0.0741\n",
      "Epoch [396/500], Training Loss: 0.0606, Validation Loss: 0.0795\n",
      "Epoch [397/500], Training Loss: 0.0624, Validation Loss: 0.0812\n",
      "Epoch [398/500], Training Loss: 0.0644, Validation Loss: 0.0772\n",
      "Epoch [399/500], Training Loss: 0.0609, Validation Loss: 0.0753\n",
      "Epoch [400/500], Training Loss: 0.0576, Validation Loss: 0.0758\n",
      "Epoch [401/500], Training Loss: 0.0573, Validation Loss: 0.0902\n",
      "Epoch [402/500], Training Loss: 0.0647, Validation Loss: 0.0778\n",
      "Epoch [403/500], Training Loss: 0.0671, Validation Loss: 0.0790\n",
      "Epoch [404/500], Training Loss: 0.0589, Validation Loss: 0.0762\n",
      "Epoch [405/500], Training Loss: 0.0621, Validation Loss: 0.0867\n",
      "Epoch [406/500], Training Loss: 0.0704, Validation Loss: 0.1001\n",
      "Epoch [407/500], Training Loss: 0.0715, Validation Loss: 0.0927\n",
      "Epoch [408/500], Training Loss: 0.0679, Validation Loss: 0.0784\n",
      "Epoch [409/500], Training Loss: 0.0599, Validation Loss: 0.0805\n",
      "Epoch [410/500], Training Loss: 0.0586, Validation Loss: 0.0748\n",
      "Epoch [411/500], Training Loss: 0.0598, Validation Loss: 0.0770\n",
      "Epoch [412/500], Training Loss: 0.0567, Validation Loss: 0.0771\n",
      "Epoch [413/500], Training Loss: 0.0619, Validation Loss: 0.0763\n",
      "Epoch [414/500], Training Loss: 0.0595, Validation Loss: 0.0746\n",
      "Epoch [415/500], Training Loss: 0.0685, Validation Loss: 0.0799\n",
      "Epoch [416/500], Training Loss: 0.0620, Validation Loss: 0.0802\n",
      "Epoch [417/500], Training Loss: 0.0591, Validation Loss: 0.0756\n",
      "Epoch [418/500], Training Loss: 0.0597, Validation Loss: 0.0793\n",
      "Epoch [419/500], Training Loss: 0.0628, Validation Loss: 0.0828\n",
      "Epoch [420/500], Training Loss: 0.0588, Validation Loss: 0.0794\n",
      "Epoch [421/500], Training Loss: 0.0604, Validation Loss: 0.0834\n",
      "Epoch [422/500], Training Loss: 0.0609, Validation Loss: 0.0757\n",
      "Epoch [423/500], Training Loss: 0.0651, Validation Loss: 0.0757\n",
      "Epoch [424/500], Training Loss: 0.0688, Validation Loss: 0.0762\n",
      "Epoch [425/500], Training Loss: 0.0579, Validation Loss: 0.0773\n",
      "Epoch [426/500], Training Loss: 0.0620, Validation Loss: 0.0779\n",
      "Epoch [427/500], Training Loss: 0.0565, Validation Loss: 0.0754\n",
      "Epoch [428/500], Training Loss: 0.0566, Validation Loss: 0.0783\n",
      "Epoch [429/500], Training Loss: 0.0622, Validation Loss: 0.0760\n",
      "Epoch [430/500], Training Loss: 0.0575, Validation Loss: 0.0770\n",
      "Epoch [431/500], Training Loss: 0.0604, Validation Loss: 0.0765\n",
      "Epoch [432/500], Training Loss: 0.0571, Validation Loss: 0.0775\n",
      "Epoch [433/500], Training Loss: 0.0590, Validation Loss: 0.0837\n",
      "Epoch [434/500], Training Loss: 0.0573, Validation Loss: 0.1069\n",
      "Epoch [435/500], Training Loss: 0.0713, Validation Loss: 0.0889\n",
      "Epoch [436/500], Training Loss: 0.0651, Validation Loss: 0.0780\n",
      "Epoch [437/500], Training Loss: 0.0627, Validation Loss: 0.0965\n",
      "Epoch [438/500], Training Loss: 0.0715, Validation Loss: 0.0802\n",
      "Epoch [439/500], Training Loss: 0.0645, Validation Loss: 0.0837\n",
      "Epoch [440/500], Training Loss: 0.0684, Validation Loss: 0.0816\n",
      "Epoch [441/500], Training Loss: 0.0587, Validation Loss: 0.0777\n",
      "Epoch [442/500], Training Loss: 0.0613, Validation Loss: 0.0748\n",
      "Epoch [443/500], Training Loss: 0.0610, Validation Loss: 0.0793\n",
      "Epoch [444/500], Training Loss: 0.0595, Validation Loss: 0.0753\n",
      "Epoch [445/500], Training Loss: 0.0576, Validation Loss: 0.0763\n",
      "Epoch [446/500], Training Loss: 0.0608, Validation Loss: 0.0821\n",
      "Epoch [447/500], Training Loss: 0.0631, Validation Loss: 0.0810\n",
      "Epoch [448/500], Training Loss: 0.0618, Validation Loss: 0.0749\n",
      "Epoch [449/500], Training Loss: 0.0628, Validation Loss: 0.0750\n",
      "Epoch [450/500], Training Loss: 0.0599, Validation Loss: 0.0804\n",
      "Epoch [451/500], Training Loss: 0.0618, Validation Loss: 0.0756\n",
      "Epoch [452/500], Training Loss: 0.0613, Validation Loss: 0.0904\n",
      "Epoch [453/500], Training Loss: 0.0685, Validation Loss: 0.0829\n",
      "Epoch [454/500], Training Loss: 0.0634, Validation Loss: 0.0844\n",
      "Epoch [455/500], Training Loss: 0.0593, Validation Loss: 0.0795\n",
      "Epoch [456/500], Training Loss: 0.0671, Validation Loss: 0.0797\n",
      "Epoch [457/500], Training Loss: 0.0643, Validation Loss: 0.0763\n",
      "Epoch [458/500], Training Loss: 0.0566, Validation Loss: 0.0849\n",
      "Epoch [459/500], Training Loss: 0.0571, Validation Loss: 0.0763\n",
      "Epoch [460/500], Training Loss: 0.0620, Validation Loss: 0.0770\n",
      "Epoch [461/500], Training Loss: 0.0736, Validation Loss: 0.0762\n",
      "Epoch [462/500], Training Loss: 0.0651, Validation Loss: 0.0761\n",
      "Epoch [463/500], Training Loss: 0.0631, Validation Loss: 0.0738\n",
      "Epoch [464/500], Training Loss: 0.0623, Validation Loss: 0.0842\n",
      "Epoch [465/500], Training Loss: 0.0666, Validation Loss: 0.0823\n",
      "Epoch [466/500], Training Loss: 0.0726, Validation Loss: 0.0975\n",
      "Epoch [467/500], Training Loss: 0.0634, Validation Loss: 0.0756\n",
      "Epoch [468/500], Training Loss: 0.0578, Validation Loss: 0.0767\n",
      "Epoch [469/500], Training Loss: 0.0612, Validation Loss: 0.0850\n",
      "Epoch [470/500], Training Loss: 0.0633, Validation Loss: 0.0931\n",
      "Epoch [471/500], Training Loss: 0.0678, Validation Loss: 0.0782\n",
      "Epoch [472/500], Training Loss: 0.0665, Validation Loss: 0.0752\n",
      "Epoch [473/500], Training Loss: 0.0590, Validation Loss: 0.0747\n",
      "Epoch [474/500], Training Loss: 0.0594, Validation Loss: 0.0818\n",
      "Epoch [475/500], Training Loss: 0.0638, Validation Loss: 0.0791\n",
      "Epoch [476/500], Training Loss: 0.0594, Validation Loss: 0.0764\n",
      "Epoch [477/500], Training Loss: 0.0588, Validation Loss: 0.0817\n",
      "Epoch [478/500], Training Loss: 0.0626, Validation Loss: 0.0796\n",
      "Epoch [479/500], Training Loss: 0.0698, Validation Loss: 0.0862\n",
      "Epoch [480/500], Training Loss: 0.0727, Validation Loss: 0.0980\n",
      "Epoch [481/500], Training Loss: 0.0650, Validation Loss: 0.0771\n",
      "Epoch [482/500], Training Loss: 0.0685, Validation Loss: 0.0746\n",
      "Epoch [483/500], Training Loss: 0.0584, Validation Loss: 0.0845\n",
      "Epoch [484/500], Training Loss: 0.0686, Validation Loss: 0.0760\n",
      "Epoch [485/500], Training Loss: 0.0624, Validation Loss: 0.0830\n",
      "Epoch [486/500], Training Loss: 0.0609, Validation Loss: 0.0839\n",
      "Epoch [487/500], Training Loss: 0.0560, Validation Loss: 0.0749\n",
      "Epoch [488/500], Training Loss: 0.0551, Validation Loss: 0.0791\n",
      "Epoch [489/500], Training Loss: 0.0631, Validation Loss: 0.0776\n",
      "Epoch [490/500], Training Loss: 0.0547, Validation Loss: 0.0773\n",
      "Epoch [491/500], Training Loss: 0.0570, Validation Loss: 0.0773\n",
      "Epoch [492/500], Training Loss: 0.0591, Validation Loss: 0.0792\n",
      "Epoch [493/500], Training Loss: 0.0597, Validation Loss: 0.0773\n",
      "Epoch [494/500], Training Loss: 0.0635, Validation Loss: 0.0800\n",
      "Epoch [495/500], Training Loss: 0.0575, Validation Loss: 0.0768\n",
      "Epoch [496/500], Training Loss: 0.0555, Validation Loss: 0.0777\n",
      "Epoch [497/500], Training Loss: 0.0545, Validation Loss: 0.0763\n",
      "Epoch [498/500], Training Loss: 0.0630, Validation Loss: 0.0757\n",
      "Epoch [499/500], Training Loss: 0.0553, Validation Loss: 0.0975\n",
      "Epoch [500/500], Training Loss: 0.0691, Validation Loss: 0.0858\n",
      "Model saved to model_artifacts/trained_model.pt\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "num_continuous_features = 2\n",
    "num_embedding_features = len(scaled_ad_copy_embeddings[0])\n",
    "num_categorical_features = 3 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_dense = features[:, :num_continuous_features + num_embedding_features]\n",
    "        x_cat = features[:, num_continuous_features + num_embedding_features:num_continuous_features + num_embedding_features + num_categorical_features]\n",
    "        outputs = model(x_dense, x_cat)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            x_dense = features[:, :num_continuous_features + num_embedding_features]\n",
    "            x_cat = features[:, num_continuous_features + num_embedding_features:num_continuous_features + num_embedding_features + num_categorical_features]\n",
    "            outputs = model(x_dense, x_cat)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
