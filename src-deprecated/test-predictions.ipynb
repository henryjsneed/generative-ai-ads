{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glaurung/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to import mlperf_logging,  No module named 'mlperf_logging'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 18:48:59.950636: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-14 18:48:59.952120: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-14 18:48:59.972535: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-14 18:48:59.972558: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-14 18:48:59.972572: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-14 18:48:59.977143: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-14 18:49:00.566771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "sys.path.append(\"/home/glaurung/ai-ads\")\n",
    "sys.path.append(\"/home/glaurung/ai-ads/dlrm\")\n",
    "from dlrm import data_utils\n",
    "import dlrm\n",
    "import pickle\n",
    "from dlrm_s_pytorch import DLRM_Net\n",
    "import numpy as np\n",
    "import ad_copy_util\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### this section is for POC model deployment. It's not useful for testing because you still need ctr for the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Create a list of 10 catchy phrases that could be used in an advertisement for a new sports drink flavor called Jungle Torrent targeting 20 year old athletes.\"\n",
    "# ad_copy_options = ['1. \"Thirst to Win with Jungle Torrent!\"', '2. \"Start your Winning Streak with Jungle Torrent!\" ', '3. \"Outperform with Jungle Torrent!\"', '4. \"Hydrate to Dominate with Jungle Torrent!\"', '5. \"Stay Energized and Go the Distance with Jungle Torrent!\"', '6. \"Recharge with Jungle Torrent!\"', '7. \"Beat Your Best with Jungle Torrent!\"', '8. \"Go Wild with Jungle Torrent!\"', '9. \"Outpace the Competition with Jungle Torrent!\"', '10. \"Unlock Your Potential with Jungle Torrent!\"']\n",
    "# #ad_copy_util.generate_ad_copy_options(prompt, max_items=10, max_tokens=300, temperature=1)\n",
    "# print(ad_copy_options)\n",
    "\n",
    "# # Use BertModel to generate embeddings instead of OpenAI API to save time and credits.\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"using \", device)\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "# ad_copy_embeddings = ad_copy_util.generate_text_embeddings(ad_copy_options, model, tokenizer, device)\n",
    "# for i, embedding in enumerate(ad_copy_embeddings):\n",
    "#     print(f\"Embedding {i+1} shape: {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare categorical test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'label_encoders/label_encoder_ad_id.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/glaurung/ai-ads/src/test-predictions.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/glaurung/ai-ads/src/test-predictions.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m label_encoders \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/glaurung/ai-ads/src/test-predictions.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mad_id\u001b[39m\u001b[39m'\u001b[39m: LabelEncoder(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/glaurung/ai-ads/src/test-predictions.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice_type\u001b[39m\u001b[39m'\u001b[39m: LabelEncoder(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glaurung/ai-ads/src/test-predictions.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mhistorical_ad_category\u001b[39m\u001b[39m'\u001b[39m: LabelEncoder()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glaurung/ai-ads/src/test-predictions.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/glaurung/ai-ads/src/test-predictions.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m df_categorical_test \u001b[39m=\u001b[39m common\u001b[39m.\u001b[39;49mtransform_with_label_encoders(label_encoders, df_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/glaurung/ai-ads/src/test-predictions.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m categorical_features \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(values) \u001b[39mfor\u001b[39;00m values \u001b[39min\u001b[39;00m df_categorical_test\u001b[39m.\u001b[39mto_numpy()]\n",
      "File \u001b[0;32m~/ai-ads/src/common.py:98\u001b[0m, in \u001b[0;36mtransform_with_label_encoders\u001b[0;34m(label_encoders, df)\u001b[0m\n\u001b[1;32m     96\u001b[0m encoded_columns \u001b[39m=\u001b[39m {}\n\u001b[1;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m column, encoder \u001b[39min\u001b[39;00m label_encoders\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlabel_encoders/label_encoder_\u001b[39;49m\u001b[39m{\u001b[39;49;00mcolumn\u001b[39m}\u001b[39;49;00m\u001b[39m.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     99\u001b[0m         loaded_encoder \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m    100\u001b[0m     encoded_columns[column \u001b[39m+\u001b[39m\n\u001b[1;32m    101\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m_encoded\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m loaded_encoder\u001b[39m.\u001b[39mtransform(df[column])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'label_encoders/label_encoder_ad_id.pkl'"
     ]
    }
   ],
   "source": [
    "label_encoders = {\n",
    "    'ad_id': LabelEncoder(),\n",
    "    'device_type': LabelEncoder(),\n",
    "    'location': LabelEncoder(),\n",
    "    'browser': LabelEncoder(),  \n",
    "    'content_category': LabelEncoder(),\n",
    "    'ad_copy': LabelEncoder(),\n",
    "    'product_type': LabelEncoder(),\n",
    "    'ad_type': LabelEncoder(),\n",
    "    'time_of_day': LabelEncoder(),\n",
    "    'day_of_week': LabelEncoder(),\n",
    "    'interaction_type': LabelEncoder(),\n",
    "    'historical_ad_category': LabelEncoder()\n",
    "}\n",
    "\n",
    "df_categorical_test = common.transform_with_label_encoders(label_encoders, df_test)\n",
    "\n",
    "categorical_features = [tuple(values) for values in df_categorical_test.to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare continuous test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "continuous_fields = ['age', 'site_visit_duration', 'time_spent_on_ad', 'pages_visited_this_session','ads_viewed_last_month', 'avg_time_spent_on_clicked_ads', 'site_visit_frequency']\n",
    "\n",
    "df_continuous = common.load_and_transform_scaler(continuous_fields, df_test)\n",
    "\n",
    "continuous_features = [tuple(values) for values in df_continuous.to_numpy()]\n",
    "\n",
    "target_feature = df_test['ad_clicked'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve ad copy embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = 'ad_copy_embeddings.pkl'\n",
    "with open(embeddings_file, 'rb') as file:\n",
    "    ad_copy_embeddings_dict = pickle.load(file)\n",
    "continuous_features_flat = common.prepare_continuous_features_with_embeddings(df_test, df_continuous, ad_copy_embeddings_dict,'ad_copy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the DLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DLRM_Net(\n",
       "  (emb_l): ModuleList(\n",
       "    (0): EmbeddingBag(84, 775, mode='sum')\n",
       "    (1): EmbeddingBag(3, 775, mode='sum')\n",
       "    (2): EmbeddingBag(5, 775, mode='sum')\n",
       "    (3): EmbeddingBag(4, 775, mode='sum')\n",
       "    (4): EmbeddingBag(6, 775, mode='sum')\n",
       "    (5): EmbeddingBag(84, 775, mode='sum')\n",
       "    (6): EmbeddingBag(6, 775, mode='sum')\n",
       "    (7-8): 2 x EmbeddingBag(4, 775, mode='sum')\n",
       "    (9): EmbeddingBag(7, 775, mode='sum')\n",
       "    (10): EmbeddingBag(3, 775, mode='sum')\n",
       "    (11): EmbeddingBag(6, 775, mode='sum')\n",
       "  )\n",
       "  (bot_l): Sequential()\n",
       "  (top_l): Sequential(\n",
       "    (0): Linear(in_features=775, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (loss_fn): BCELoss()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_cardinalities = []\n",
    "\n",
    "# Loop through each category feature and calculate the cardinality\n",
    "for column in label_encoders.keys():\n",
    "    cardinality = len(df_test[column].unique())\n",
    "    category_cardinalities.append(cardinality)\n",
    "\n",
    "category_cardinalities_array = np.array(category_cardinalities)\n",
    "\n",
    "# embedding_sizes: the sizes of the embedding tables based on the cardinalities of the categorical features\n",
    "ln_emb = category_cardinalities_array\n",
    "\n",
    "# original number of continuous features\n",
    "original_m_spa = np.array(continuous_features[0]).shape[0]\n",
    "\n",
    "# size of each ad copy embedding\n",
    "ad_copy_embedding_size = 768  \n",
    "\n",
    "# m_spa is the size of each embedding\n",
    "m_spa = original_m_spa + ad_copy_embedding_size\n",
    "\n",
    "ln_bot = np.array([m_spa])\n",
    "\n",
    "# ln_top = np.array([m_spa + embedding_size * len(categorical_features[0]), 16, 1])\n",
    "ln_top = np.array([775, 16, 1])\n",
    "\n",
    "device = \"cpu\"\n",
    "model = DLRM_Net(\n",
    "    m_spa,\n",
    "    ln_emb,\n",
    "    ln_bot,\n",
    "    ln_top,\n",
    "    arch_interaction_op=\"dot\",\n",
    "    sigmoid_bot=-1,\n",
    "    sigmoid_top=len(ln_top) - 2,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"/home/glaurung/ai-ads/trained_model.pt\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = torch.tensor(categorical_features, dtype=torch.long)\n",
    "X_cont = torch.tensor(continuous_features_flat, dtype=torch.float32)\n",
    "Y = torch.tensor(target_feature, dtype=torch.float32).view(-1, 1)\n",
    "dataset = TensorDataset(X_cont, X_cat, Y)\n",
    "\n",
    "# Create dataset and data loader\n",
    "test_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.6914\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "test_loss = 0.0\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "with torch.no_grad():\n",
    "    for x_cont, x_cat, y in test_loader:\n",
    "        lS_o, lS_i = common.generate_offsets_and_indices_per_feature(x_cat)\n",
    "        y_pred = model(x_cont, lS_o, lS_i)\n",
    "        loss = criterion(y_pred, y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Average Test Loss: {avg_test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
